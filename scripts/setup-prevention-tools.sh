#!/bin/bash
# Setup script for Neo-Legacy Prevention Tools

echo "üöÄ Setting up Pipeline Modernization Prevention Tools"
echo "=================================================="

# 1. Install pre-commit hooks
echo "üìã Setting up pre-commit hooks..."
cp hooks/pre-commit-pipeline-check.py .git/hooks/pre-commit-pipeline-check
chmod +x .git/hooks/pre-commit-pipeline-check

# Create main pre-commit hook
cat > .git/hooks/pre-commit << 'EOF'
#!/bin/bash
# Main pre-commit hook with pipeline quality checks

echo "üîç Running pipeline quality checks..."
python3 .git/hooks/pre-commit-pipeline-check

exit_code=$?

if [ $exit_code -ne 0 ]; then
    echo ""
    echo "üí° Quick fixes available in VS Code:"
    echo "   ‚Ä¢ Right-click file ‚Üí 'Pipeline: Quick Fix'"
    echo "   ‚Ä¢ Use code lens suggestions"
    echo "   ‚Ä¢ Chat with AI: 'Fix these issues'"
    echo ""
fi

exit $exit_code
EOF

chmod +x .git/hooks/pre-commit

# 2. Setup VS Code workspace settings for prevention
echo "‚öôÔ∏è  Configuring VS Code for real-time prevention..."
mkdir -p .vscode
cat > .vscode/settings.json << 'EOF'
{
  "pipelineModernizer.preventionMode": true,
  "pipelineModernizer.realTimeAnalysis": true,
  "pipelineModernizer.blockingPatterns": [
    "sequential_requests_in_loop",
    "sleep_in_lambda",
    "missing_error_handling"
  ],
  "pipelineModernizer.complexityThreshold": 6,
  "pipelineModernizer.showLearningTips": true,
  "pipelineModernizer.teamStandards": {
    "enforceAsyncPatterns": true,
    "requireCtxParameter": true,
    "preferModernPackages": true
  }
}
EOF

# 3. Create pipeline templates
echo "üìÅ Creating pipeline templates..."
mkdir -p templates/pipeline

cat > templates/pipeline/modern_pipeline.py << 'EOF'
"""
Modern Pipeline Template - Prevents Legacy Patterns
Generated by Pipeline Modernizer Extension
"""
from typing import Dict, Any, List
import asyncio
import httpx
import polars as pl
from dataclasses import dataclass
import logging

# Setup logging
logger = logging.getLogger(__name__)

@dataclass 
class PipelineContext:
    """Standard context object for pipeline stages"""
    config: Dict[str, Any]
    data: Any = None
    metadata: Dict[str, Any] = None
    errors: List[str] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.errors is None:
            self.errors = []

def pipeline_decorator(func):
    """Decorator for pipeline stages with error handling and monitoring"""
    async def wrapper(ctx: PipelineContext) -> PipelineContext:
        stage_name = func.__name__
        logger.info(f"Starting stage: {stage_name}")
        
        try:
            result = await func(ctx)
            logger.info(f"Completed stage: {stage_name}")
            return result
            
        except Exception as e:
            logger.error(f"Failed stage: {stage_name} - {str(e)}")
            ctx.errors.append(f"{stage_name}: {str(e)}")
            raise
            
    return wrapper

@pipeline_decorator
async def prepare(ctx: PipelineContext) -> PipelineContext:
    """
    Prepare stage - Setup and validation
    
    Best practices enforced:
    - Input validation
    - Configuration setup  
    - Resource initialization
    """
    logger.info("üîß Preparing pipeline execution")
    
    # Validate required config
    required_keys = ['source_url', 'output_path']  # Customize as needed
    for key in required_keys:
        if key not in ctx.config:
            raise ValueError(f"Missing required config: {key}")
    
    # Initialize metadata
    ctx.metadata['start_time'] = asyncio.get_event_loop().time()
    ctx.metadata['stage'] = 'prepare'
    
    return ctx

@pipeline_decorator  
async def fetch(ctx: PipelineContext) -> PipelineContext:
    """
    Fetch stage - Parallel data retrieval
    
    Best practices enforced:
    - Async HTTP requests with httpx
    - Proper error handling
    - Connection pooling
    - Retry logic
    """
    logger.info("üåê Fetching data")
    
    urls = ctx.config.get('urls', [])
    if not urls:
        logger.warning("No URLs provided for fetching")
        return ctx
    
    async with httpx.AsyncClient() as client:
        # Parallel requests instead of sequential
        tasks = []
        for url in urls:
            task = fetch_single_url(client, url)
            tasks.append(task)
        
        # Execute all requests concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results and handle errors
        successful_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                ctx.errors.append(f"Failed to fetch {urls[i]}: {str(result)}")
                logger.error(f"Failed to fetch {urls[i]}: {str(result)}")
            else:
                successful_results.append(result)
        
        ctx.data = successful_results
        ctx.metadata['fetched_count'] = len(successful_results)
    
    return ctx

async def fetch_single_url(client: httpx.AsyncClient, url: str) -> Dict[str, Any]:
    """Fetch single URL with retry logic"""
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            response = await client.get(url, timeout=30)
            response.raise_for_status()
            
            return {
                'url': url,
                'status': response.status_code,
                'data': response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text,
                'headers': dict(response.headers)
            }
            
        except httpx.RequestError as e:
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(2 ** attempt)  # Exponential backoff

@pipeline_decorator
async def transform(ctx: PipelineContext) -> PipelineContext:
    """
    Transform stage - Business logic and data processing
    
    Best practices enforced:
    - Use polars for data processing (faster than pandas)
    - Vectorized operations
    - Memory-efficient processing
    """
    logger.info("üîÑ Transforming data")
    
    if not ctx.data:
        logger.warning("No data to transform")
        return ctx
    
    # Example transformation with polars (modern alternative to pandas)
    try:
        # Convert to polars DataFrame for efficient processing
        df = pl.DataFrame(ctx.data)
        
        # Example transformations - customize as needed
        df_transformed = (
            df
            .filter(pl.col("status") == 200)  # Filter successful requests
            .with_columns([
                pl.col("data").str.len_chars().alias("data_length"),
                pl.lit("processed").alias("processing_status")
            ])
            .group_by("url")
            .agg([
                pl.col("data_length").mean().alias("avg_length"),
                pl.col("status").count().alias("request_count")
            ])
        )
        
        ctx.data = df_transformed.to_dicts()
        ctx.metadata['transformed_records'] = len(ctx.data)
        
    except Exception as e:
        logger.error(f"Transformation failed: {str(e)}")
        raise
    
    return ctx

@pipeline_decorator
async def save(ctx: PipelineContext) -> PipelineContext:
    """
    Save stage - Data persistence
    
    Best practices enforced:
    - Batch operations
    - Data validation before save
    - Proper error handling
    - Atomic operations
    """
    logger.info("üíæ Saving results")
    
    if not ctx.data:
        logger.warning("No data to save")
        return ctx
    
    output_path = ctx.config.get('output_path')
    if not output_path:
        raise ValueError("No output_path specified in config")
    
    try:
        # Use polars for efficient saving
        df = pl.DataFrame(ctx.data)
        
        # Save in efficient format
        if output_path.endswith('.parquet'):
            df.write_parquet(output_path)
        elif output_path.endswith('.csv'):
            df.write_csv(output_path)
        else:
            # Default to JSON
            df.write_json(output_path)
        
        ctx.metadata['saved_records'] = len(ctx.data)
        ctx.metadata['output_path'] = output_path
        
        logger.info(f"Successfully saved {len(ctx.data)} records to {output_path}")
        
    except Exception as e:
        logger.error(f"Save failed: {str(e)}")
        raise
    
    return ctx

async def run_pipeline(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main pipeline execution function
    
    This orchestrates the entire pipeline following the
    Prepare-Fetch-Transform-Save pattern
    """
    logger.info("üöÄ Starting pipeline execution")
    
    # Initialize context
    ctx = PipelineContext(config=config)
    
    try:
        # Execute pipeline stages in order
        ctx = await prepare(ctx)
        ctx = await fetch(ctx)
        ctx = await transform(ctx)
        ctx = await save(ctx)
        
        # Calculate execution time
        end_time = asyncio.get_event_loop().time()
        execution_time = end_time - ctx.metadata['start_time']
        
        logger.info(f"‚úÖ Pipeline completed successfully in {execution_time:.2f}s")
        
        return {
            'success': True,
            'execution_time': execution_time,
            'records_processed': ctx.metadata.get('saved_records', 0),
            'metadata': ctx.metadata,
            'errors': ctx.errors
        }
        
    except Exception as e:
        logger.error(f"‚ùå Pipeline failed: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'metadata': ctx.metadata,
            'errors': ctx.errors
        }

# Example usage
if __name__ == "__main__":
    # Example configuration
    config = {
        'urls': [
            'https://api.example.com/data/1',
            'https://api.example.com/data/2',
            'https://api.example.com/data/3'
        ],
        'output_path': 'output/results.parquet'
    }
    
    # Run pipeline
    result = asyncio.run(run_pipeline(config))
    print(f"Pipeline result: {result}")
EOF

# 4. Setup development workflow integration
echo "üîó Setting up development workflow integration..."

cat > scripts/new-pipeline.sh << 'EOF'
#!/bin/bash
# Create new pipeline from template

PIPELINE_NAME=$1
if [ -z "$PIPELINE_NAME" ]; then
    echo "Usage: $0 <pipeline_name>"
    exit 1
fi

PIPELINE_DIR="pipelines/${PIPELINE_NAME}"
mkdir -p "$PIPELINE_DIR"

# Copy template
cp templates/pipeline/modern_pipeline.py "${PIPELINE_DIR}/main.py"

# Generate basic config
cat > "${PIPELINE_DIR}/config.yaml" << EOL
# Pipeline configuration for ${PIPELINE_NAME}
name: ${PIPELINE_NAME}
description: "Generated pipeline following modern patterns"

# Data sources
sources:
  - type: api
    url: "https://api.example.com/data"
    
# Processing settings  
processing:
  batch_size: 1000
  parallel_workers: 5
  
# Output settings
output:
  format: parquet
  path: "output/${PIPELINE_NAME}/"
  
# AWS settings (if deploying to Lambda)
aws:
  memory: 1024
  timeout: 300
  environment: development
EOL

echo "‚úÖ Created new pipeline: ${PIPELINE_DIR}"
echo "üìù Next steps:"
echo "   1. Edit ${PIPELINE_DIR}/main.py for your business logic"
echo "   2. Update ${PIPELINE_DIR}/config.yaml with your settings"
echo "   3. Test locally: python ${PIPELINE_DIR}/main.py"
echo "   4. VS Code will provide real-time guidance as you code!"
EOF

chmod +x scripts/new-pipeline.sh

# 5. Setup team standards configuration
echo "üìä Setting up team standards..."
cat > .pipeline-standards.yaml << 'EOF'
# Team Pipeline Standards Configuration
# Used by VS Code extension for real-time guidance

patterns:
  required:
    - prepare_fetch_transform_save: true
    - async_functions: true  
    - ctx_parameter: true
    - error_handling: true
    
  forbidden:
    - sequential_http_requests: true
    - time_sleep_in_lambda: true
    - pandas_iterrows: true
    - hardcoded_urls: true
    
packages:
  recommended:
    - httpx: "Use instead of requests for async support"
    - polars: "Use instead of pandas for 5x performance"
    - orjson: "Use instead of json for speed"
    - pydantic: "Use for data validation"
    
  discouraged:
    - requests: "Use httpx for async support"
    - urllib: "Use httpx for better API"
    - xml.etree: "Use lxml for performance"
    
complexity:
  max_function_complexity: 6
  max_file_complexity: 15
  max_lines_per_function: 50
  
aws:
  lambda:
    max_memory: 3008
    max_timeout: 900
    cold_start_optimization: true
    
monitoring:
  required_logging: true
  performance_tracking: true
  error_tracking: true
EOF

echo ""
echo "‚úÖ Setup complete! Neo-Legacy Prevention Tools are now active."
echo ""
echo "üéØ What's different now:"
echo "   ‚Ä¢ Pre-commit hooks block legacy patterns"
echo "   ‚Ä¢ VS Code provides real-time guidance while coding"
echo "   ‚Ä¢ New pipelines start with modern templates"
echo "   ‚Ä¢ Team standards enforced automatically"
echo ""
echo "üöÄ Try it out:"
echo "   1. Run: ./scripts/new-pipeline.sh my-test-pipeline"
echo "   2. Open the generated file in VS Code"
echo "   3. Try typing legacy patterns - watch the guidance!"
echo "   4. Attempt to commit legacy code - see the prevention!"
echo ""
echo "üí° The extension will now coach developers toward modern patterns"
echo "   instead of just fixing legacy code after it's written!"